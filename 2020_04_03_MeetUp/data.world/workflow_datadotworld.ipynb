{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datadotworld example flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's the problem?\n",
    "When BPIO is processing data in-house, the best option is going to be direct SQL queries with the `pyodbc` Python package. That provides faster access to the freshest possible data on facilities and fleet activity. But we aren't going to be handing out our database credentials to everyone who's involved in a data partnership with us, or who just wants to explore a bit of DGS data. \n",
    "\n",
    "For external partners, analysts, and collaborators, the `datadotworld` Software Development Kit for Python offers a clean alternative. We break off tables that can be saved as .csv or .xslx files, host them at our page on the [Data.World site](https://data.world/dgsbpio), and partners can gain access to them without us needing to pass them files. This has several advantages:\n",
    "\n",
    "- The lack of data files enables cleaner GitHub repositories and workflows.\n",
    "- We can update the data in one place and all partners will have the update, without us needing to send files to each of them.\n",
    "- The data dictionary uploads along with the data, so we can be sure everybody has the same column definitions.\n",
    "\n",
    "The code below demonstrates some of the features and flows we can expect from data served by data.world.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datadotworld as dw  # here's the Python SDK\n",
    "import pandas as pd  # at DGS, we typically pair datadotworld with Pandas\n",
    "import pprint as pp\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the SDK if needed\n",
    "This library requires a data.world API authentication token to work.\n",
    "\n",
    "Your authentication token can be obtained on data.world once you enable Python under Integrations > Python\n",
    "\n",
    "To configure the library, run the following command. You will be prompted to provide your API key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dw configure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic data import\n",
    "### Load dataset\n",
    "The `dw.load_dataset()` function is probably the one that partners would use the most, if they simply want to use Data.World to obtain data. \n",
    "\n",
    "Take a look at the structure of the dictionary returned by the `describe()` method on a datadotworld dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'description': 'This dataset contains .csv files exported from Archibus. This '\n",
      "                'is meant to be a toy dataset for demonstration purposes.',\n",
      " 'homepage': 'https://data.world/dgsbpio/facilities-sandbox',\n",
      " 'name': 'dgsbpio_facilities-sandbox',\n",
      " 'resources': [{'format': 'csv',\n",
      "                'name': 'buildings',\n",
      "                'path': 'data/buildings.csv'},\n",
      "               {'format': 'csv',\n",
      "                'name': 'work_requests',\n",
      "                'path': 'data/work_requests.csv'},\n",
      "               {'bytes': 304730,\n",
      "                'description': 'This is the buildings table, where each row '\n",
      "                               'represents a unique building. \\n'\n",
      "                               '\\n'\n",
      "                               'Data sourced from DGS Archibus on March 9, '\n",
      "                               '2020.',\n",
      "                'format': 'csv',\n",
      "                'keywords': ['raw data'],\n",
      "                'mediatype': 'text/csv',\n",
      "                'name': 'original/buildings.csv',\n",
      "                'path': 'original/buildings.csv'},\n",
      "               {'bytes': 171979,\n",
      "                'description': 'Small subset of table `wr` in DGS Archibus. '\n",
      "                               'Columns have been selected to avoid publishing '\n",
      "                               'PII. \\n'\n",
      "                               '\\n'\n",
      "                               'Data uploaded on March 9, 2020.',\n",
      "                'format': 'csv',\n",
      "                'keywords': ['raw data'],\n",
      "                'mediatype': 'text/csv',\n",
      "                'name': 'original/work_requests.csv',\n",
      "                'path': 'original/work_requests.csv'}],\n",
      " 'title': 'Facilities Sandbox'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/james/anaconda3/envs/BPIO/lib/python3.6/site-packages/datadotworld/datadotworld.py:194: UserWarning: You are using an outdated copy of dgsbpio/facilities-sandbox. If you wish to use the latest version, call this function with the argument auto_update=True or force_update=True\n",
      "  'force_update=True'.format(dataset_key))\n"
     ]
    }
   ],
   "source": [
    "facilities = dw.load_dataset(dataset_key='dgsbpio/facilities-sandbox')\n",
    "pp.pprint(facilities.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import a specific table for analysis\n",
    "#### ... as a QueryResults object\n",
    "We can use the SDK's `query()` function to grab whole tables ... or selections or combination of tables based on a SQL query. This returns a QueryResults object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datadotworld.models.query.QueryResults"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wr = dw.query(dataset_key='dgsbpio/facilities-sandbox', \n",
    "              query='SELECT * FROM work_requests')\n",
    "\n",
    "type(wr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main advantage of keeping the data as a `QueryResults` object is that the column descriptions are all accessible through its `describe()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fields': [{'description': 'Unique identifier for each building; PK in table '\n",
      "                            '`bl`.',\n",
      "             'name': 'bl_id',\n",
      "             'rdfType': None,\n",
      "             'type': 'string'},\n",
      "            {'description': 'Unique identifier for each work request; PK in '\n",
      "                            'table `wr`',\n",
      "             'name': 'wr_id',\n",
      "             'rdfType': None,\n",
      "             'type': 'string'},\n",
      "            {'description': 'Categorical variable representing the type of '\n",
      "                            'issue the request refers to.',\n",
      "             'name': 'prob_type',\n",
      "             'rdfType': None,\n",
      "             'type': 'string'},\n",
      "            {'description': 'Date when work request was created, always by a '\n",
      "                            'person. Please ignore time-related section of '\n",
      "                            'timestamp.',\n",
      "             'format': 'any',\n",
      "             'name': 'date_requested',\n",
      "             'rdfType': 'http://www.w3.org/2001/XMLSchema#dateTime',\n",
      "             'type': 'datetime'},\n",
      "            {'description': 'Date when request was approved and sent to '\n",
      "                            'dispatcher, which is usually done automatically. '\n",
      "                            'Please ignore time-related section of timestamp.',\n",
      "             'format': 'any',\n",
      "             'name': 'date_assigned',\n",
      "             'rdfType': 'http://www.w3.org/2001/XMLSchema#dateTime',\n",
      "             'type': 'datetime'},\n",
      "            {'description': 'Date when request was marked completed, always by '\n",
      "                            'a person. Please ignore time-related section of '\n",
      "                            'timestamp.',\n",
      "             'format': 'any',\n",
      "             'name': 'date_completed',\n",
      "             'rdfType': 'http://www.w3.org/2001/XMLSchema#dateTime',\n",
      "             'type': 'datetime'},\n",
      "            {'description': 'Categorical variable representing the team the '\n",
      "                            'work request was assigned to. ',\n",
      "             'name': 'work_team_id',\n",
      "             'rdfType': None,\n",
      "             'type': 'string'}]}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(wr.describe(), depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ... as a Pandas DataFrame\n",
    "But what if we want to just analyze the data in Pandas?\n",
    "\n",
    "A `QueryResults` object returned by `query()` has a `.dataframe`Â attribute that causes the return value to be a Pandas DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wr_df = wr.dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bl_id</th>\n",
       "      <th>wr_id</th>\n",
       "      <th>prob_type</th>\n",
       "      <th>date_requested</th>\n",
       "      <th>date_assigned</th>\n",
       "      <th>date_completed</th>\n",
       "      <th>work_team_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B00027</td>\n",
       "      <td>19566</td>\n",
       "      <td>OVERHDDOOR</td>\n",
       "      <td>2015-01-02</td>\n",
       "      <td>2015-01-02</td>\n",
       "      <td>2015-06-23</td>\n",
       "      <td>XYZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00020</td>\n",
       "      <td>19585</td>\n",
       "      <td>OVERHDDOOR</td>\n",
       "      <td>2015-01-05</td>\n",
       "      <td>2015-01-05</td>\n",
       "      <td>2015-06-23</td>\n",
       "      <td>CONTRACT                                      ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B04031</td>\n",
       "      <td>19756</td>\n",
       "      <td>OVERHDDOOR</td>\n",
       "      <td>2015-01-07</td>\n",
       "      <td>2015-01-07</td>\n",
       "      <td>2015-06-23</td>\n",
       "      <td>CONTRACT                                      ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B04016</td>\n",
       "      <td>19757</td>\n",
       "      <td>OVERHDDOOR</td>\n",
       "      <td>2015-01-07</td>\n",
       "      <td>2015-01-07</td>\n",
       "      <td>2015-05-13</td>\n",
       "      <td>CONTRACT                                      ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B00166</td>\n",
       "      <td>19800</td>\n",
       "      <td>HVAC</td>\n",
       "      <td>2015-01-08</td>\n",
       "      <td>2015-01-08</td>\n",
       "      <td>NaT</td>\n",
       "      <td>DISPATCHERS                                   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      bl_id  wr_id                         prob_type date_requested  \\\n",
       "0  B00027    19566  OVERHDDOOR                           2015-01-02   \n",
       "1  B00020    19585  OVERHDDOOR                           2015-01-05   \n",
       "2  B04031    19756  OVERHDDOOR                           2015-01-07   \n",
       "3  B04016    19757  OVERHDDOOR                           2015-01-07   \n",
       "4  B00166    19800  HVAC                                 2015-01-08   \n",
       "\n",
       "  date_assigned date_completed  \\\n",
       "0    2015-01-02     2015-06-23   \n",
       "1    2015-01-05     2015-06-23   \n",
       "2    2015-01-07     2015-06-23   \n",
       "3    2015-01-07     2015-05-13   \n",
       "4    2015-01-08            NaT   \n",
       "\n",
       "                                        work_team_id  \n",
       "0                                                XYZ  \n",
       "1  CONTRACT                                      ...  \n",
       "2  CONTRACT                                      ...  \n",
       "3  CONTRACT                                      ...  \n",
       "4  DISPATCHERS                                   ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wr_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the numerical ID columns, which we saved, correctly, as _strings_ in the Data.World browser-based UI, are still strings when we import them into a Pandas DataFrame here. **That is awesome!** Similarly, the datetime columns remain in the correct format, and the user of the Python SDK does not need to coerce them into the correct data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bl_id                     object\n",
       "wr_id                     object\n",
       "prob_type                 object\n",
       "date_requested    datetime64[ns]\n",
       "date_assigned     datetime64[ns]\n",
       "date_completed    datetime64[ns]\n",
       "work_team_id              object\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wr_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the API client to upload data\n",
    "The idea of using an API to push data up to Data.World originally got the Baltimore DGS team pretty excited because we hoped to be able to use GitHub as a familiar space to store all the data descriptions, labels, column descriptions, and so on. Then, if we were interested in applying version control to this information, that would just work out of the box.  \n",
    "\n",
    "It turns out, though, that Data.World's API is still a bit limited. Only file labels (e.g. \"clean data\", \"raw data\", \"documentation\") and a file-level description can be created in this way. The following cells demonstrate this functionality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fire up a client object\n",
    "client = dw.api_client()\n",
    "path = Path.cwd() / 'data' / 'buildings.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty dictionary\n",
    "metadata = {}\n",
    "# put some labels and a description into the dictionary\n",
    "metadata['buildings.csv'] =  { 'labels': ['raw data'], \n",
    "                              'description': 'the file description for this file'}\n",
    "\n",
    "# upload the data to the DGS sandbox!\n",
    "client.upload_files(dataset_key='dgsbpio/facilities-sandbox',\n",
    "                    files=[path], \n",
    "                    files_metadata=metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look at the methods and attributions of the client object shows us that the API can do lots of things, most of which we haven't tried yet. Options include:\n",
    "\n",
    "- appending rows to an existing table\n",
    "- adding or deleting datasets, insights, and projects\n",
    "- syncing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_RestApiClient__build_dataset_obj',\n",
       " '_RestApiClient__build_insight_obj',\n",
       " '_RestApiClient__build_project_obj',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_build_api_client',\n",
       " '_config',\n",
       " '_datasets_api',\n",
       " '_download_api',\n",
       " '_download_host',\n",
       " '_host',\n",
       " '_insights_api',\n",
       " '_projects_api',\n",
       " '_protocol',\n",
       " '_streams_api',\n",
       " '_uploads_api',\n",
       " '_user_api',\n",
       " 'add_files_via_url',\n",
       " 'add_linked_dataset',\n",
       " 'append_records',\n",
       " 'create_dataset',\n",
       " 'create_insight',\n",
       " 'create_project',\n",
       " 'delete_dataset',\n",
       " 'delete_files',\n",
       " 'delete_insight',\n",
       " 'delete_project',\n",
       " 'download_datapackage',\n",
       " 'download_dataset',\n",
       " 'download_file',\n",
       " 'fetch_contributing_datasets',\n",
       " 'fetch_contributing_projects',\n",
       " 'fetch_datasets',\n",
       " 'fetch_liked_datasets',\n",
       " 'fetch_liked_projects',\n",
       " 'fetch_projects',\n",
       " 'get_dataset',\n",
       " 'get_insight',\n",
       " 'get_insights_for_project',\n",
       " 'get_project',\n",
       " 'get_user_data',\n",
       " 'remove_linked_dataset',\n",
       " 'replace_dataset',\n",
       " 'replace_insight',\n",
       " 'replace_project',\n",
       " 'sparql',\n",
       " 'sql',\n",
       " 'sync_files',\n",
       " 'update_dataset',\n",
       " 'update_insight',\n",
       " 'update_project',\n",
       " 'upload_file',\n",
       " 'upload_files']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(client)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
